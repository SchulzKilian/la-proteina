name: local_latents_transformer

output_parameterization:
  bb_ca: v
  local_latents: v

# 6 instead of 14 layers
nlayers: 6  

# Stays the same to not retrain autoencoder
token_dim: 768  

# Stays the same to not change transformer performance 
nheads: 12  

parallel_mha_transition: False
strict_feats: False

# Sequence features
feats_seq: ["xt_bb_ca", "xt_local_latents", "x_sc_bb_ca", "x_sc_local_latents", "optional_ca_coors_nm_seq_feat", "optional_res_type_seq_feat"]
feats_cond_seq: ["time_emb_bb_ca", "time_emb_local_latents"]

# Parameters for the features we extract
dim_cond: 128  # Reduced from 256
idx_emb_dim: 128  # Reduced from 256
t_emb_dim: 128  # Reduced from 256

# Pair representation features
feats_pair_repr: ["rel_seq_sep", "xt_bb_ca_pair_dists", "x_sc_bb_ca_pair_dists", "optional_ca_pair_dist"]
feats_pair_cond: ["time_emb_bb_ca", "time_emb_local_latents"]

# Binning for the pair distances
xt_pair_dist_dim: 30
xt_pair_dist_min: 0.1
xt_pair_dist_max: 3
x_sc_pair_dist_dim: 30
x_sc_pair_dist_min: 0.1
x_sc_pair_dist_max: 3
seq_sep_dim: 127

# Reduced pair dimension: 128 instead of 256. 
# This drastically reduces the memory and compute overhead of the N^2 attention bias.
pair_repr_dim: 128

update_pair_repr: False
update_pair_repr_every_n: 3
use_tri_mult: False
use_downsampling: False
use_qkln: True